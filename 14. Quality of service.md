### Quality of service

- best effort
- burstable
- gauranteed

#### Best effort
- In yaml file, there is no mention about CPU and memory
- Kubernates try to give its best, but no cpu and memory value commited
- suitable for lest priority pods

  Note: if node get stress, besteffort pods will move to another node by default

```
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
spec:
  containers:
  - name: container
    image: nginx
    resources: {}
```

#### Burstable
- In yaml file, there is mention about CPU and memory
- Request is gauranteed, and it is given at the time of pod startup
- Request is used by the scheduler
- Limit is the maximum value, It can be used at the time of load increase
- if the pod claims beyond the maximium value, It will be burst, thats why it is called burstable, limit alwasys should be > requests
- Suitable for second priority pods

```
vim burst.yaml
```
  
```
apiVersion: v1
kind: Pod
metadata:
  name: burst
  labels:
    name: apache
    managed-by: Charlie
    version: "2.4"
    component: frontend
    part-of: webserver
    instance: jan2023
    env: prod
    cleant: abc
    business: eco
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
```

```
kubectl apply -f burst.yaml
kubectl get pods
kubectl describe pod burst
```

#### Gauranteed
- In yaml file, there is mention about CPU and memory
- limit = requests
- Suitable for highest priority pods
- Usage: Suitable for critical applications where resource requirements are well-known and need to be strictly enforced.

```
vim guaranteed.yaml
```


```
  apiVersion: v1
kind: Pod
metadata:
  name: guaranteed
  labels:
    name: apache
    managed-by: Charlie
    version: "2.4"
    component: frontend
    part-of: webserver
    instance: jan2023
    env: prod
    cleant: abc
    business: eco
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 200m
        memory: 256Mi
```

```
kubectl apply -f guaranteed.yaml
kubectl get pods
kubectl describe pod guaranteed
```

#### What happend if load goes beyond limits
- when cpu exceed- cpu throtteling will be activated, then process will be slow down
- when memory exceed - kubernates call the OOM killer, then container will be restarted



#### Stress check

```
vim stress.yaml
```


```
apiVersion: v1
kind: Pod
metadata:
  name: stress
  labels:
   app: nnappone
spec:
  containers:
    - name: networknuts-app
      image: lovelearnlinux/stress:latest
      resources:
        limits:
          memory: "250Mi"
        requests:
          memory: "150Mi"
      command: ["stress"]
      args: ["--vm", "1", "--vm-bytes", "241M", "--vm-hang", "1"]
```

```
kubectl top nodes
```

![image](https://github.com/mahimanew/Kubernates/assets/24412769/ce22640f-b26e-4d01-b6d8-479a843699c0)

Let us install the metric server to check the top node list

```
vim metric.yaml
kubectl apply -f metric.yaml

# check it
kubectl -n kube-system get pods
kubectl top nodes
kubectl top pods
```
![image](https://github.com/mahimanew/Kubernates/assets/24412769/7a3d1a79-ee9e-4d8a-ba35-86f581983509)

![image](https://github.com/mahimanew/Kubernates/assets/24412769/fe212640-7f0d-485e-8c08-1b7701ed4465)

Current ram consumption is 7mb for burstable pod

let us apply the stress.yaml file

```
kubectl apply -f stress.yaml
kubectl get pods
kubectl top pods
```

![image](https://github.com/mahimanew/Kubernates/assets/24412769/ba11c7da-56e4-43bd-b373-95cc51c130c7)

Let us change the request value more than limit 

```
# delete the previous stress file and create new one
kubectl delete -f stress.yaml
vim stress.yaml

#change 245 to 255M

kubectl apply -f stress.yaml
kubectl get pods
```
![image](https://github.com/mahimanew/Kubernates/assets/24412769/e694b0a3-15ee-4a39-9d48-ba3d9f25fe39)








